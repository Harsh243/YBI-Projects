{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "MSfaSry6hvaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f56086a7-4953-44e8-e0d2-2bdb37319446"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import chardet\n"
      ],
      "metadata": {
        "id": "j00Cri2FJb0T"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel('/content/Input.xlsx')"
      ],
      "metadata": {
        "id": "PTN5LCnSHk_s"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/positive-words.txt', 'rb') as file:\n",
        "    encoding = chardet.detect(file.read())['encoding']\n",
        "    file.seek(0)\n",
        "    positive_words = file.read().decode(encoding).splitlines()\n",
        "\n",
        "with open('/content/negative-words.txt', 'rb') as file:\n",
        "    encoding = chardet.detect(file.read())['encoding']\n",
        "    file.seek(0)\n",
        "    negative_words = file.read().decode(encoding).splitlines()"
      ],
      "metadata": {
        "id": "EmpCurovJfsZ"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "def load_stopwords(file_path):\n",
        "    with open(file_path, 'r', encoding='latin-1') as file:\n",
        "        return set(word.strip() for word in file)\n",
        "\n",
        "stop_words_auditor = load_stopwords('/content/StopWords_Auditor.txt')\n",
        "stop_words_currencies = load_stopwords('/content/StopWords_Currencies.txt')\n",
        "stop_words_dates_and_numbers = load_stopwords('/content/StopWords_DatesandNumbers.txt')\n",
        "stop_words_generic = load_stopwords('/content/StopWords_Generic.txt')\n",
        "stop_words_genericlong = load_stopwords('/content/StopWords_GenericLong.txt')\n",
        "stop_words_geographic = load_stopwords('/content/StopWords_Geographic.txt')\n",
        "stop_words_names = load_stopwords('/content/StopWords_Names.txt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6yNwLz_HuOg",
        "outputId": "2b226e30-dcea-46c6-bdc6-93532bc74ddc"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocessing function to clean text\n",
        "def preprocess_text(text):\n",
        "    # Remove HTML tags\n",
        "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
        "    # Tokenize text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    # Remove stopwords and punctuation\n",
        "    clean_tokens = [word.lower() for word in tokens if word.lower() not in stop_words_auditor and word.isalnum()]\n",
        "    return clean_tokens\n"
      ],
      "metadata": {
        "id": "urpncRa5HwtV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to extract text from websites\n",
        "def extract_text(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    return soup.get_text()"
      ],
      "metadata": {
        "id": "zKSp8AWaHzUB"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate positive score\n",
        "def calculate_positive_score(text):\n",
        "    return sum(1 for word in text if word in positive_words)\n",
        "\n",
        "# Function to calculate negative score\n",
        "def calculate_negative_score(text):\n",
        "    return sum(1 for word in text if word in negative_words)\n",
        "\n",
        "# Function to count complex words\n",
        "def count_complex_words(text):\n",
        "    return sum(1 for word in text if syllable_count(word) > 2)\n",
        "\n",
        "# Function to calculate syllable count for a word\n",
        "def syllable_count(word):\n",
        "    # Implement syllable counting logic\n",
        "    # Example: Simplified logic for demonstration\n",
        "    return len(re.findall(r'[aeiouAEIOU]+', word))\n",
        "\n",
        "# Function to calculate metrics and scores\n",
        "def calculate_metrics(text):\n",
        "    positive_score = calculate_positive_score(text)\n",
        "    negative_score = calculate_negative_score(text)\n",
        "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
        "    subjectivity_score = (positive_score + negative_score) / (len(text) + 0.000001)\n",
        "    avg_sentence_length = len(text) / text.count('.')\n",
        "    percentage_complex_words = count_complex_words(text) / len(text)\n",
        "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "    avg_words_per_sentence = len(text) / len(nltk.sent_tokenize(text))\n",
        "    complex_word_count = count_complex_words(text)\n",
        "    word_count = len(text)\n",
        "    personal_pronouns = sum(1 for word in text if word.lower() in ['i', 'we', 'my', 'ours', 'us'])\n",
        "    avg_word_length = sum(len(word) for word in text) / len(text)\n",
        "    return polarity_score, subjectivity_score, avg_sentence_length, percentage_complex_words, fog_index, avg_words_per_sentence, complex_word_count, word_count, personal_pronouns, avg_word_length\n",
        "\n"
      ],
      "metadata": {
        "id": "RRoxq0pwH4Ub"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in data.iterrows():\n",
        "    # Extract text from URL\n",
        "    text = extract_text(row['URL'])\n",
        "    # Preprocess text\n",
        "    cleaned_text = preprocess_text(text)\n",
        "\n",
        "    # Calculate scores and metrics\n",
        "    positive_score = calculate_positive_score(cleaned_text)\n",
        "    negative_score = calculate_negative_score(cleaned_text)\n",
        "\n",
        "    # Avoid division by zero by checking if denominator is zero\n",
        "    if (positive_score + negative_score) != 0:\n",
        "        polarity_score = (positive_score - negative_score) / (positive_score + negative_score)\n",
        "        subjectivity_score = (positive_score + negative_score) / len(cleaned_text)\n",
        "    else:\n",
        "        polarity_score = 0\n",
        "        subjectivity_score = 0\n",
        "\n",
        "    avg_sentence_length = len(cleaned_text) / (cleaned_text.count('.') + 1) if len(cleaned_text) != 0 else 0\n",
        "    percentage_complex_words = count_complex_words(cleaned_text) / len(cleaned_text) if len(cleaned_text) != 0 else 0\n",
        "    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)\n",
        "    avg_words_per_sentence = len(cleaned_text) / (cleaned_text.count('.') + 1) if len(cleaned_text) != 0 else 0\n",
        "    complex_word_count = count_complex_words(cleaned_text)\n",
        "    word_count = len(cleaned_text)\n",
        "    personal_pronouns = sum(1 for word in cleaned_text if word.lower() in ['i', 'we', 'my', 'ours', 'us'])\n",
        "    avg_word_length = sum(len(word) for word in cleaned_text) / len(cleaned_text) if len(cleaned_text) != 0 else 0\n",
        "\n",
        "    # Assign calculated scores and metrics to DataFrame\n",
        "    data.at[index, 'Positive_Score'] = positive_score\n",
        "    data.at[index, 'Negative_Score'] = negative_score\n",
        "    data.at[index, 'Polarity_Score'] = polarity_score\n",
        "    data.at[index, 'Subjectivity_Score'] = subjectivity_score\n",
        "    data.at[index, 'Avg_Sentence_Length'] = avg_sentence_length\n",
        "    data.at[index, 'Percentage_of_Complex_Words'] = percentage_complex_words\n",
        "    data.at[index, 'FOG_Index'] = fog_index\n",
        "    data.at[index, 'Avg_Words_Per_Sentence'] = avg_words_per_sentence\n",
        "    data.at[index, 'Complex_Word_Count'] = complex_word_count\n",
        "    data.at[index, 'Word_Count'] = word_count\n",
        "    data.at[index, 'Personal_Pronouns'] = personal_pronouns\n",
        "    data.at[index, 'Avg_Word_Length'] = avg_word_length\n"
      ],
      "metadata": {
        "id": "EpmNZX4oH7Ns"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results to Excel\n",
        "data.to_excel('output_file.xlsx', index=False)"
      ],
      "metadata": {
        "id": "tCac-zysH8-S"
      },
      "execution_count": 42,
      "outputs": []
    }
  ]
}